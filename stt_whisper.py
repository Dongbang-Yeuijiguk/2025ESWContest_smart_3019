import os, sys, time, math, queue, threading, tempfile, uuid
import numpy as np
import sounddevice as sd
import soundfile as sf
import webrtcvad
from faster_whisper import WhisperModel
from gate import TTS_PLAYING

# ===== ì„¤ì • =====
SAMPLE_RATE   = 16000
CHANNELS      = 1
FRAME_MS      = int(os.getenv("FRAME_MS", "20"))  # WebRTC VAD: 10, 20, 30msë§Œ ì§€ì›
FRAME_SAMPLES = SAMPLE_RATE * FRAME_MS // 1000
RING_TARGET   = 512

MODEL_SIZE    = os.getenv("WHISPER_MODEL", "base")
DEVICE        = os.getenv("WHISPER_DEVICE", "cpu")
COMPUTE       = os.getenv("WHISPER_COMPUTE", "int8")
LANG          = os.getenv("WHISPER_LANG", "ko")

# VAD íŒŒë¼ë¯¸í„° - 2ë‹¨ê³„ ì‹œìŠ¤í…œì— ë§ê²Œ ìµœì í™”
VAD_MODE      = int(os.getenv("VAD_MODE", "3"))       # 0~3, 3ì´ ê°€ì¥ aggressive
MIN_SPEECH_MS = int(os.getenv("MIN_SPEECH_MS", "150"))  # Wake wordìš©ìœ¼ë¡œ ë‹¨ì¶•
MIN_SIL_MS    = int(os.getenv("MIN_SIL_MS", "200"))    # ë” ë¹ ë¥¸ ë°˜ì‘
PRE_SPEECH_MS = int(os.getenv("PRE_SPEECH_MS", "100")) # ì „ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•

MIN_SPEECH_FR = max(1, MIN_SPEECH_MS // FRAME_MS)
MIN_SIL_FR    = max(1, MIN_SIL_MS // FRAME_MS)
PRE_SPEECH_FR = max(0, PRE_SPEECH_MS // FRAME_MS)

# ì†ë„ ìµœì í™” ì„¤ì •
MAX_AUDIO_LENGTH = int(os.getenv("MAX_AUDIO_LENGTH", "6"))  # Wake wordìš©ìœ¼ë¡œ ë‹¨ì¶•
BEAM_SIZE = int(os.getenv("BEAM_SIZE", "1"))               # ë¹ ë¥¸ ì²˜ë¦¬
BEST_OF = int(os.getenv("BEST_OF", "1"))                   # ë¹ ë¥¸ ì²˜ë¦¬

# ë””ë²„ê·¸/ì¥ì¹˜/í”„ë¦¬ì•°í”„/ìš°íšŒ
DEBUG         = os.getenv("VAD_DEBUG", "1") == "1"
DEVICE_INDEX  = int(os.getenv("SD_INPUT_DEV", "-1"))  # -1ì´ë©´ ê¸°ë³¸
AMP_DB        = float(os.getenv("AMP_DB", "6"))       # ì†Œí”„íŠ¸ í”„ë¦¬ì•°í”„ (dB)
GAIN          = float(10 ** (AMP_DB / 20.0))
BYPASS_VAD    = os.getenv("BYPASS_VAD", "0") == "1"   # VAD ìš°íšŒ: 0
SEG_MS        = int(os.getenv("SEG_MS", "1000"))      # ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸
GAP_MS        = int(os.getenv("GAP_MS", "200"))       # ì§§ì€ ê°„ê²©

# ë…¸ì´ì¦ˆ ê²Œì´íŠ¸ (WebRTC VAD ë³´ì™„ìš©)
NOISE_GATE_DB = float(os.getenv("NOISE_GATE_DB", "-60"))  # dBFS ê¸°ì¤€

# í í¬ê¸° ìµœì í™” - 2ë‹¨ê³„ ì‹œìŠ¤í…œìš©
audio_q   = queue.Queue(maxsize=50)    # ë” ì‘ì€ ë²„í¼
segment_q = queue.Queue(maxsize=10)    # ë” ì‘ì€ ë²„í¼
stop_flag = threading.Event()

# ëª¨ë¸ ìºì‹± - ë§¤ë²ˆ ë¡œë“œí•˜ì§€ ì•Šë„ë¡
_model_cache = None
_model_lock = threading.Lock()

# íŒŒì´í”„ë¼ì¸ ì—°ê²° - 2ë‹¨ê³„ ì‹œìŠ¤í…œìš©
_subscribers = []

# TTS ì”í–¥(ëˆ„í™”) ì°¨ë‹¨ - ë” ê°•í™”
REFRACTORY_SEC = 0.8  # ì¡°ê¸ˆ ë” ê¸¸ê²Œ
_last_tts_seen = 0.0

def tts_blocked() -> bool:
    """TTS ì¤‘ì´ê±°ë‚˜ ì”í–¥ ì°½ì´ë©´ True - 2ë‹¨ê³„ ì‹œìŠ¤í…œìš© ê°•í™”"""
    global _last_tts_seen
    if TTS_PLAYING.is_set():
        _last_tts_seen = time.monotonic()
        return True
    return (time.monotonic() - _last_tts_seen) < REFRACTORY_SEC

def subscribe(callback):
    """STT ê²°ê³¼ë¥¼ ë°›ì„ ì½œë°± í•¨ìˆ˜ ë“±ë¡"""
    _subscribers.append(callback)

def unsubscribe(callback):
    """ì½œë°± í•¨ìˆ˜ ë“±ë¡ í•´ì œ"""
    if callback in _subscribers:
        _subscribers.remove(callback)

def clear_subscribers():
    """ëª¨ë“  êµ¬ë…ì ì œê±°"""
    _subscribers.clear()

def _notify_subscribers(text: str):
    """ë“±ë¡ëœ ëª¨ë“  ì½œë°±ì— í…ìŠ¤íŠ¸ ì „ë‹¬"""
    for callback in _subscribers:
        try:
            callback(text)
        except Exception as e:
            print(f"[WARN] ì½œë°± ì˜¤ë¥˜: {e}")

def get_model():
    """ëª¨ë¸ ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ìºì‹±"""
    global _model_cache
    with _model_lock:
        if _model_cache is None:
            print(f"[LOAD] faster-whisper model={MODEL_SIZE}, device={DEVICE}, compute={COMPUTE}")
            _model_cache = WhisperModel(MODEL_SIZE, device=DEVICE, compute_type=COMPUTE)
            
            # ì›Œë°ì—… - ì²« ì‹¤í–‰ ì§€ì—° ë°©ì§€ (ë” ì‘ì€ ìƒ˜í”Œë¡œ)
            print("[WARMUP] ëª¨ë¸ ì›Œë°ì—… ì¤‘...")
            dummy_audio = np.random.randn(SAMPLE_RATE // 2).astype(np.float32) * 0.01  # 0.5ì´ˆ ìƒ˜í”Œ
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                tmp_path = tmp.name
                sf.write(tmp_path, dummy_audio, SAMPLE_RATE, subtype="PCM_16")
            
            try:
                parts, _ = _model_cache.transcribe(tmp_path, language=LANG, beam_size=1, best_of=1)
                list(parts)  # ì‹¤ì œ ì‹¤í–‰
                print("[WARMUP] ì™„ë£Œ")
            except:
                pass
            finally:
                try: os.remove(tmp_path)
                except: pass
    
    return _model_cache

# ===== ìœ í‹¸ =====
def dbfs_from_int16(x: np.ndarray) -> float:
    if x.size == 0: return -120.0
    rms = np.sqrt(np.mean((x.astype(np.float32))**2))
    return 20*math.log10(max(rms/32767.0, 1e-12))

def dbfs_from_float(x: np.ndarray) -> float:
    if x.size == 0: return -120.0
    rms = np.sqrt(np.mean(x.astype(np.float32)**2))
    return 20*math.log10(max(rms, 1e-12))

def load_webrtc_vad():
    """WebRTC VAD ì´ˆê¸°í™”"""
    global FRAME_MS, FRAME_SAMPLES

    # WebRTC VADëŠ” 10ms, 20ms, 30ms í”„ë ˆì„ë§Œ ì§€ì›
    if FRAME_MS not in [10, 20, 30]:
        print(f"[WARN] WebRTC VADëŠ” 10/20/30ms í”„ë ˆì„ë§Œ ì§€ì›. {FRAME_MS}ms â†’ 20msë¡œ ë³€ê²½")
        FRAME_MS = 20
        FRAME_SAMPLES = SAMPLE_RATE * FRAME_MS // 1000
    
    vad = webrtcvad.Vad()
    vad.set_mode(VAD_MODE)  # 0: ê°€ì¥ ê´€ëŒ€, 3: ê°€ì¥ ì—„ê²©
    print(f"[VAD] WebRTC VAD ì´ˆê¸°í™” ì™„ë£Œ (mode={VAD_MODE}, frame={FRAME_MS}ms)")
    return vad

# ===== ë§ˆì´í¬ =====
def mic_stream():
    level_buf = []; cb_count = 0

    # ì¥ì¹˜ ì •ë³´ ë¡œê·¸
    try:
        devinfo = sd.query_devices(DEVICE_INDEX if DEVICE_INDEX>=0 else None)
        print(f"[MIC] using device: {devinfo['name']} (index={DEVICE_INDEX})")
    except Exception as e:
        print(f"[MIC] device query failed: {e}")

    def callback(indata, frames, time_info, status):
        nonlocal cb_count

        # ğŸ”’ TTS ì¤‘(ë˜ëŠ” ì”í–¥ ì°½) â†’ í”„ë ˆì„/í ì¦‰ì‹œ ë“œë¡­ (ê°•í™”ëœ ì°¨ë‹¨)
        if tts_blocked():
            # ëŒ€ê¸°ì—´ê¹Œì§€ ì‹¹ ë¹„ì›Œì„œ ë°€ë¦° í”„ë ˆì„ ì œê±°
            try:
                while True:
                    audio_q.get_nowait()
            except queue.Empty:
                pass
            return

        if status: print(f"[AUDIO]{status}", file=sys.stderr)
        data = indata[:,0] if indata.ndim==2 else indata  # float32 -1..1
        
        # ì†Œí”„íŠ¸ í”„ë¦¬ì•°í”„
        if AMP_DB != 0:
            data = np.clip(data * GAIN, -1.0, 1.0)
            
        # ìª¼ê°œì„œ íì— - í ê°€ë“ ì°¬ ê²½ìš° ë“œë¡­í•˜ì—¬ ì§€ì—° ë°©ì§€
        for i in range(0, len(data), FRAME_SAMPLES):
            frame = data[i:i+FRAME_SAMPLES]
            if len(frame)==FRAME_SAMPLES:
                try: 
                    audio_q.put_nowait(frame.copy())
                except queue.Full: 
                    # íê°€ ê°€ë“ ì°¬ ê²½ìš° ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
                    try: audio_q.get_nowait()
                    except queue.Empty: pass
                    try: audio_q.put_nowait(frame.copy())
                    except queue.Full: pass
                    
        cb_count += 1
        if DEBUG and cb_count % 20 == 0:
            level_buf.append(data.copy())
            z = np.concatenate(level_buf) if level_buf else data
            db = dbfs_from_float(z)
            bar = "#"*min(50, max(0,int((db+60)/60*50)))
            print(f"{db:6.1f} dB {bar}")
            level_buf.clear()

    with sd.InputStream(device=DEVICE_INDEX if DEVICE_INDEX>=0 else None,
                        channels=CHANNELS, samplerate=SAMPLE_RATE,
                        dtype='float32', callback=callback, blocksize=FRAME_SAMPLES):
        while not stop_flag.is_set():
            sd.sleep(50)

# ===== WebRTC VAD ì„¸ê·¸ë¨¼í„° (2ë‹¨ê³„ ì‹œìŠ¤í…œì— ìµœì í™”) =====
def vad_segmenter(vad):
    if BYPASS_VAD:
        return bypass_segmenter()   # ìš°íšŒ ëª¨ë“œ
    
    # ìˆœí™˜ ë²„í¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
    from collections import deque
    pre_buf = deque(maxlen=PRE_SPEECH_FR)
    seg_buf = []
    speech_count, sil_count = 0, 0
    in_speech = False
    
    print("[INFO] 2ë‹¨ê³„ WebRTC VAD ëŒ€ê¸° ì¤‘â€¦ Wake wordë¥¼ ë§í•˜ì„¸ìš”.")
    
    while not stop_flag.is_set():
        # TTS ì°¨ë‹¨ ê°•í™”
        if tts_blocked():
            # ì§„í–‰ ì¤‘ì¸ ì„¸ê·¸ë¨¼íŠ¸/ë²„í¼ ì „ë¶€ ë¦¬ì…‹
            pre_buf.clear(); seg_buf.clear()
            speech_count = sil_count = 0
            in_speech = False
            # ë§ˆì´í¬ ëˆ„ì  í”„ë ˆì„ë„ ë¹„ì›Œì„œ ì§€ì—° ì œê±°
            try:
                while True:
                    audio_q.get_nowait()
            except queue.Empty:
                pass
            time.sleep(0.01)
            continue
            
        try: 
            frame = audio_q.get(timeout=0.05)  # ë” ë¹ ë¥¸ ì‘ë‹µ
        except queue.Empty: 
            continue

        # ë…¸ì´ì¦ˆ ê²Œì´íŠ¸ ì²´í¬ (WebRTC VAD ë³´ì™„)
        db_frame = dbfs_from_float(frame)
        if db_frame < NOISE_GATE_DB:
            is_speech = False
        else:
            # WebRTC VADëŠ” int16 PCM ë°ì´í„°ê°€ í•„ìš”
            frame_int16 = np.int16(np.clip(frame, -1.0, 1.0) * 32767)
            frame_bytes = frame_int16.tobytes()
            
            try:
                is_speech = vad.is_speech(frame_bytes, SAMPLE_RATE)
            except Exception as e:
                if DEBUG:
                    print(f"[VAD] WebRTC VAD ì˜¤ë¥˜: {e}")
                is_speech = False

        # ë¬¸ë‘ ë²„í¼ ê´€ë¦¬ - deque ì‚¬ìš©ìœ¼ë¡œ ìë™ í¬ê¸° ê´€ë¦¬
        pre_buf.append(frame)

        # VAD ìƒíƒœë¨¸ì‹  - 2ë‹¨ê³„ ì‹œìŠ¤í…œì— ë§ê²Œ ì¡°ì •
        if is_speech:
            sil_count = 0
            speech_count += 1
            
            # ìŒì„± ì‹œì‘ ê°ì§€ (ë” ë¹ ë¥¸ ë°˜ì‘)
            if not in_speech and speech_count >= 1:
                seg_buf.extend(pre_buf)
                in_speech = True
                if DEBUG:
                    print(f"[VAD] >>> START (level={db_frame:.1f}dBFS)")
            
            seg_buf.append(frame)
            
        else:  # ì¹¨ë¬µ
            speech_count = 0
            
            if in_speech:
                sil_count += 1
                seg_buf.append(frame)
                
                # ì¶©ë¶„í•œ ì¹¨ë¬µìœ¼ë¡œ ìŒì„± ì¢…ë£Œ (ë” ë¹ ë¥¸ ë°˜ì‘)
                if sil_count >= MIN_SIL_FR:
                    seg = np.concatenate(seg_buf).astype(np.float32)
                    
                    # ê¸¸ì´ ì œí•œìœ¼ë¡œ ì²˜ë¦¬ ì†ë„ í–¥ìƒ (2ë‹¨ê³„ ì‹œìŠ¤í…œìš©)
                    max_samples = MAX_AUDIO_LENGTH * SAMPLE_RATE
                    if seg.size > max_samples:
                        seg = seg[-max_samples:]  # ë’·ë¶€ë¶„ë§Œ ì‚¬ìš©
                    
                    # ìµœì†Œ ìŒì„± ê¸¸ì´ ì²´í¬ (ë” ê´€ëŒ€í•˜ê²Œ)
                    if seg.size > 0 and len(seg_buf) >= MIN_SPEECH_FR:
                        seg_i16 = np.int16(np.clip(seg, -1.0, 1.0) * 32767)
                        
                        if DEBUG:
                            dur = int(seg.size * 1000 / SAMPLE_RATE)
                            lvl = dbfs_from_int16(seg_i16)
                            print(f"[VAD] <<< END dur={dur}ms level={lvl:.1f}dBFS")
                        
                        try: 
                            segment_q.put_nowait(seg_i16)
                        except queue.Full: 
                            # íê°€ ê°€ë“ ì°¬ ê²½ìš° ì˜¤ë˜ëœ ì„¸ê·¸ë¨¼íŠ¸ ì œê±°
                            try: segment_q.get_nowait()
                            except queue.Empty: pass
                            try: segment_q.put_nowait(seg_i16)
                            except queue.Full: pass
                    
                    # ìƒíƒœ ì´ˆê¸°í™”
                    seg_buf.clear()
                    in_speech = False
                    sil_count = 0

# ===== ìš°íšŒ ì„¸ê·¸ë¨¼í„° (2ë‹¨ê³„ ì‹œìŠ¤í…œìš©) =====
def bypass_segmenter():
    print("[INFO] BYPASS_VAD=1 â†’ ê³ ì • êµ¬ê°„ìœ¼ë¡œ ë°”ë¡œ STT ë³´ëƒ„ (2ë‹¨ê³„ ëª¨ë“œ)")
    buf = []
    seg_len = max(1, SEG_MS // FRAME_MS)      # í”„ë ˆì„ ë‹¨ìœ„
    gap_len = max(0, GAP_MS // FRAME_MS)
    
    while not stop_flag.is_set():
        try: frame = audio_q.get(timeout=0.05)  # ë” ë¹ ë¥¸ ì‘ë‹µ
        except queue.Empty: continue
        
        buf.append(frame)
        if len(buf) >= seg_len:
            seg = np.concatenate(buf).astype(np.float32)
            buf.clear()
            
            if gap_len:  # ì•½ê°„ì˜ ê°„ê²©
                for _ in range(gap_len):
                    try: audio_q.get_nowait()
                    except queue.Empty: break
                    
            seg_i16 = np.int16(np.clip(seg, -1.0, 1.0) * 32767)
            try: segment_q.put_nowait(seg_i16)
            except queue.Full: 
                # íê°€ ê°€ë“ ì°¬ ê²½ìš° ì˜¤ë˜ëœ ì„¸ê·¸ë¨¼íŠ¸ ì œê±°
                try: segment_q.get_nowait()
                except queue.Empty: pass
                try: segment_q.put_nowait(seg_i16)
                except queue.Full: pass

# ===== ìµœì í™”ëœ STT ì›Œì»¤ (2ë‹¨ê³„ ì‹œìŠ¤í…œìš©) =====
def stt_worker():
    model = get_model()  # ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©
    
    while not stop_flag.is_set():
        try: seg = segment_q.get(timeout=0.05)  # ë” ë¹ ë¥¸ ì‘ë‹µ
        except queue.Empty: continue
        
        # ğŸ”’ TTS ì¤‘/ì”í–¥ ì°½ â†’ ì„¸ê·¸ë¨¼íŠ¸ íê¸° (ê°•í™”ëœ ì°¨ë‹¨)
        if tts_blocked():
            segment_q.task_done() if hasattr(segment_q, "task_done") else None
            continue

        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp_path = tmp.name
            sf.write(tmp_path, seg, SAMPLE_RATE, subtype="PCM_16")
        
        try:
            t0=time.time()
            
            # ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ì¶”ë¡  (2ë‹¨ê³„ ì‹œìŠ¤í…œìš©)
            parts, info = model.transcribe(
                tmp_path, 
                language=LANG, 
                vad_filter=False,  # VADëŠ” ì´ë¯¸ ì ìš©ë¨
                beam_size=BEAM_SIZE,
                best_of=BEST_OF,
                temperature=0.0,   # ê²°ì •ë¡ ì  ë””ì½”ë”©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
                condition_on_previous_text=False  # ì´ì „ í…ìŠ¤íŠ¸ ì˜ì¡´ì„± ì œê±°
            )
            
            text="".join([p.text for p in parts]).strip()
            
            # ê¸°ì¡´ ì¶œë ¥ (2ë‹¨ê³„ ì‹œìŠ¤í…œ í‘œì‹œ)
            print("\n----- [2-STAGE STT RESULT] -----")
            print(text if text else "(ë¹ˆ ê²°ê³¼)")
            print(f"[latency] {(time.time()-t0)*1000:.0f} ms | [level] {dbfs_from_int16(seg):.1f} dBFS")
            print("--------------------------------")
            
            # êµ¬ë…ìë“¤ì—ê²Œ ì•Œë¦¼ (íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì „ë‹¬)
            if text:
                _notify_subscribers(text)
                
        except Exception as e:
            print(f"[STT] ì˜¤ë¥˜: {e}")
        finally:
            try: os.remove(tmp_path)
            except: pass

def transcribe_file(path: str):
    """ì˜¤ë””ì˜¤ íŒŒì¼ í•˜ë‚˜ ë°›ì•„ì„œ Whisperë¡œ STT (íŒŒì¼ ëª¨ë“œ)"""
    if not os.path.exists(path):
        print(f"[ERR] íŒŒì¼ ì—†ìŒ: {path}")
        return ""

    print(f"[FILE] ì…ë ¥ íŒŒì¼: {path}")
    model = get_model()  # ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©

    # wav/mp3 ê°™ì€ í¬ë§·ì„ ì „ë¶€ soundfileë¡œ ë¡œë“œ
    audio, sr = sf.read(path, dtype="float32")
    if audio.ndim > 1:
        audio = audio[:,0]  # ëª¨ë…¸ë¡œ
    if sr != SAMPLE_RATE:
        try:
            import librosa
            audio = librosa.resample(audio, orig_sr=sr, target_sr=SAMPLE_RATE)
            sr = SAMPLE_RATE
        except ImportError:
            print("[WARN] librosa ì—†ìŒ. ë¦¬ìƒ˜í”Œë§ ìƒëµ.")

    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ STT (faster-whisperëŠ” íŒŒì¼ ê²½ë¡œì—ì„œ ë” ì•ˆì •ì )
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp_path = tmp.name
        sf.write(tmp_path, audio, SAMPLE_RATE, subtype="PCM_16")

    try:
        segs, info = model.transcribe(
            tmp_path, 
            language=LANG, 
            vad_filter=True,
            beam_size=BEAM_SIZE,
            best_of=BEST_OF,
            temperature=0.0
        )
        text = "".join([s.text for s in segs]).strip()
        print("\n===== FILE STT RESULT =====")
        print(text if text else "(ë¹ˆ ê²°ê³¼)")
        print("============================")
        return text
    finally:
        try: os.remove(tmp_path)
        except: pass

# ===== ìƒíƒœ ê´€ë¦¬ í•¨ìˆ˜ë“¤ (2ë‹¨ê³„ ì‹œìŠ¤í…œìš©) =====
def get_stats():
    """STT ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜"""
    return {
        "audio_queue_size": audio_q.qsize(),
        "segment_queue_size": segment_q.qsize(),
        "subscribers_count": len(_subscribers),
        "tts_blocked": tts_blocked(),
        "model_loaded": _model_cache is not None
    }

def clear_queues():
    """ëª¨ë“  í ë¹„ìš°ê¸°"""
    try:
        while True:
            audio_q.get_nowait()
    except queue.Empty:
        pass
    
    try:
        while True:
            segment_q.get_nowait()
    except queue.Empty:
        pass

# ===== ë©”ì¸ =====
def main():
    if len(sys.argv) > 1:
        # ì¸ìë¡œ íŒŒì¼ ë“¤ì–´ì˜¤ë©´ íŒŒì¼ ëª¨ë“œ
        path = sys.argv[1]
        transcribe_file(path)
    else:
        # ì•„ë‹ˆë©´ ì‹¤ì‹œê°„ ëª¨ë“œ (2ë‹¨ê³„ ì‹œìŠ¤í…œ)
        try:
            vad = None if BYPASS_VAD else load_webrtc_vad()
            t_mic=threading.Thread(target=mic_stream,daemon=True)
            t_vad=threading.Thread(target=vad_segmenter,args=(vad,),daemon=True)
            t_stt=threading.Thread(target=stt_worker,daemon=True)
            t_mic.start(); t_vad.start(); t_stt.start()
            print("[START] 2ë‹¨ê³„ ëŒ€í™”í˜• WebRTC VAD + STT ì‹œì‘. Ctrl+C ì¢…ë£Œ.")
            print("[INFO] Wake word â†’ ëª…ë ¹ì–´ ìˆœì„œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            while True: time.sleep(0.5)
        except KeyboardInterrupt:
            print("\n[STOP] ì¢…ë£Œ ì¤‘â€¦"); stop_flag.set(); time.sleep(0.5)

if __name__=="__main__":
    main()
